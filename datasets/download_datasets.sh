#!/bin/bash

# Dataset Download Script for Trident
# This script downloads and prepares benchmark datasets for vector search experiments

set -e  # Exit on error

SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
DATASET_DIR="${SCRIPT_DIR}"

echo "Dataset directory: ${DATASET_DIR}"

# Function to download and extract SIFT dataset
download_sift() {
    echo "Downloading SIFT1M dataset..."
    mkdir -p "${DATASET_DIR}/siftsmall"
    cd "${DATASET_DIR}/siftsmall"

    # SIFT1M dataset from INRIA
    wget -nc ftp://ftp.irisa.fr/local/texmex/corpus/siftsmall.tar.gz
    tar -xzf siftsmall.tar.gz

    echo "SIFT dataset downloaded successfully"
    echo "Files: base.fvecs, query.fvecs, groundtruth.ivecs"
}

# Function to download NFCorpus dataset
download_nfcorpus() {
    echo "Downloading NFCorpus dataset..."
    mkdir -p "${DATASET_DIR}/nfcorpus"
    cd "${DATASET_DIR}/nfcorpus"

    # NFCorpus is from BEIR benchmark
    echo "NFCorpus dataset requires BEIR benchmark installation"
    echo "Please install BEIR and use the following Python code:"
    echo ""
    echo "from beir import util"
    echo "from beir.datasets.data_loader import GenericDataLoader"
    echo "dataset = 'nfcorpus'"
    echo "url = f'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset}.zip'"
    echo "data_path = util.download_and_unzip(url, '.')"
    echo ""
    echo "You'll also need to generate embeddings using a model like sentence-transformers"
}

# Function to download TripClick dataset
download_tripclick() {
    echo "Downloading TripClick dataset..."
    mkdir -p "${DATASET_DIR}/tripclick"
    cd "${DATASET_DIR}/tripclick"

    # TripClick dataset
    echo "TripClick dataset is available from Microsoft Research"
    echo "Download from: https://tripleclick.github.io/"
    echo ""
    echo "After downloading, you'll need to:"
    echo "1. Process the query-document pairs"
    echo "2. Generate embeddings using your chosen model"
    echo "3. Convert to .fvecs format"
}

# Function to download LAION subset
download_laion() {
    echo "Downloading LAION dataset subset..."
    mkdir -p "${DATASET_DIR}/laion"
    cd "${DATASET_DIR}/laion"

    echo "LAION dataset is very large. You can:"
    echo "1. Download from: https://laion.ai/blog/laion-5b/"
    echo "2. Use LAION-400M or LAION-5B subsets"
    echo "3. Generate embeddings using CLIP or similar models"
    echo ""
    echo "For a small subset for testing, you can use the img2dataset tool:"
    echo "pip install img2dataset"
}

# Function to download Triples dataset
download_triples() {
    echo "Downloading MS MARCO Triples dataset..."
    mkdir -p "${DATASET_DIR}/triples"
    cd "${DATASET_DIR}/triples"

    echo "MS MARCO Triples dataset:"
    echo "Download from: https://microsoft.github.io/msmarco/"
    echo ""
    echo "You'll need:"
    echo "1. collection.tsv (documents)"
    echo "2. queries.train.tsv (training queries)"
    echo "3. qidpidtriples.train.full.tsv (query-doc triples)"
    echo ""
    echo "Generate embeddings using a pre-trained model like:"
    echo "- sentence-transformers/msmarco-distilbert-base-v4"
    echo "- sentence-transformers/all-MiniLM-L6-v2"
}

# Function to create README
create_readme() {
    cat > "${DATASET_DIR}/README.md" << 'EOF'
# Datasets for Trident

This directory contains scripts to download and prepare benchmark datasets for Trident experiments.

## Available Datasets

### 1. SIFT1M / SIFTSmall
- **Source**: http://corpus-texmex.irisa.fr/
- **Type**: Image feature vectors (128-dim)
- **Size**: 10K vectors (SIFTSmall), 1M vectors (SIFT1M)
- **Use**: Standard ANN benchmark

### 2. NFCorpus
- **Source**: BEIR benchmark
- **Type**: Medical domain text embeddings
- **Size**: ~3.6K documents, 323 queries
- **Use**: Domain-specific retrieval

### 3. TripClick
- **Source**: https://tripleclick.github.io/
- **Type**: Web search query-document pairs
- **Size**: Large-scale web search data
- **Use**: Web search evaluation

### 4. LAION
- **Source**: https://laion.ai/
- **Type**: Image-text pairs with CLIP embeddings
- **Size**: Various subsets available
- **Use**: Multimodal retrieval

### 5. MS MARCO Triples
- **Source**: https://microsoft.github.io/msmarco/
- **Type**: Passage ranking dataset
- **Size**: 8.8M passages
- **Use**: Neural ranking benchmark

## Usage

```bash
# Download specific dataset
./download_datasets.sh sift
./download_datasets.sh nfcorpus
./download_datasets.sh tripclick
./download_datasets.sh laion
./download_datasets.sh triples

# Download all datasets
./download_datasets.sh all
```

## Data Format

Datasets should be converted to the following format:
- `base.fvecs`: Base vectors (database)
- `query.fvecs`: Query vectors
- `gt.ivecs`: Ground truth nearest neighbors
- `neighbors.bin`: HNSW graph neighbors (generated by index-builder)
- `nodes.bin`: Vector data in binary format (generated by index-builder)

## Preprocessing

After downloading raw data, use the provided preprocessing scripts:

```python
# Example: Convert embeddings to .fvecs format
import numpy as np
import struct

def write_fvecs(filename, vectors):
    with open(filename, 'wb') as f:
        for vec in vectors:
            dim = len(vec)
            f.write(struct.pack('i', dim))
            f.write(struct.pack('f' * dim, *vec))

# Load your embeddings
embeddings = np.load('your_embeddings.npy')
write_fvecs('base.fvecs', embeddings)
```

## Notes

- Some datasets require additional processing to generate embeddings
- You may need to install sentence-transformers, BEIR, or other libraries
- Ensure you have sufficient disk space (some datasets are very large)
- Check the license terms for each dataset before use
EOF
    echo "README.md created in ${DATASET_DIR}"
}

# Main script
show_usage() {
    echo "Usage: $0 [dataset_name|all]"
    echo ""
    echo "Available datasets:"
    echo "  sift       - SIFT1M/SIFTSmall dataset"
    echo "  nfcorpus   - NFCorpus dataset (BEIR)"
    echo "  tripclick  - TripClick dataset"
    echo "  laion      - LAION image-text dataset"
    echo "  triples    - MS MARCO Triples dataset"
    echo "  all        - Download all datasets"
    echo ""
    echo "Example: $0 sift"
}

# Parse command line arguments
if [ $# -eq 0 ]; then
    show_usage
    create_readme
    exit 0
fi

case "$1" in
    sift)
        download_sift
        ;;
    nfcorpus)
        download_nfcorpus
        ;;
    tripclick)
        download_tripclick
        ;;
    laion)
        download_laion
        ;;
    triples)
        download_triples
        ;;
    all)
        download_sift
        download_nfcorpus
        download_tripclick
        download_laion
        download_triples
        ;;
    *)
        echo "Unknown dataset: $1"
        show_usage
        exit 1
        ;;
esac

create_readme
echo ""
echo "Dataset download process completed!"
echo "Please check ${DATASET_DIR} for instructions and data files."
