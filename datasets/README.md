# Datasets for Trident

This directory contains scripts to download and prepare benchmark datasets for Trident experiments.

## Available Datasets

### 1. SIFT1M / SIFTSmall
- **Source**: http://corpus-texmex.irisa.fr/
- **Type**: Image feature vectors (128-dim)
- **Size**: 10K vectors (SIFTSmall), 1M vectors (SIFT1M)
- **Use**: Standard ANN benchmark

### 2. NFCorpus
- **Source**: BEIR benchmark
- **Type**: Medical domain text embeddings
- **Size**: ~3.6K documents, 323 queries
- **Use**: Domain-specific retrieval

### 3. TripClick
- **Source**: https://tripleclick.github.io/
- **Type**: Web search query-document pairs
- **Size**: Large-scale web search data
- **Use**: Web search evaluation

### 4. LAION
- **Source**: https://laion.ai/
- **Type**: Image-text pairs with CLIP embeddings
- **Size**: Various subsets available
- **Use**: Multimodal retrieval

### 5. MS MARCO Triples
- **Source**: https://microsoft.github.io/msmarco/
- **Type**: Passage ranking dataset
- **Size**: 8.8M passages
- **Use**: Neural ranking benchmark

## Usage

```bash
# Download specific dataset
./download_datasets.sh sift
./download_datasets.sh nfcorpus
./download_datasets.sh tripclick
./download_datasets.sh laion
./download_datasets.sh triples

# Download all datasets
./download_datasets.sh all
```

## Data Format

Datasets should be converted to the following format:
- `base.fvecs`: Base vectors (database)
- `query.fvecs`: Query vectors
- `gt.ivecs`: Ground truth nearest neighbors
- `neighbors.bin`: HNSW graph neighbors (generated by index-builder)
- `nodes.bin`: Vector data in binary format (generated by index-builder)

## Preprocessing

After downloading raw data, use the provided preprocessing scripts:

```python
# Example: Convert embeddings to .fvecs format
import numpy as np
import struct

def write_fvecs(filename, vectors):
    with open(filename, 'wb') as f:
        for vec in vectors:
            dim = len(vec)
            f.write(struct.pack('i', dim))
            f.write(struct.pack('f' * dim, *vec))

# Load your embeddings
embeddings = np.load('your_embeddings.npy')
write_fvecs('base.fvecs', embeddings)
```

## Notes

- Some datasets require additional processing to generate embeddings
- You may need to install sentence-transformers, BEIR, or other libraries
- Ensure you have sufficient disk space (some datasets are very large)
- Check the license terms for each dataset before use
